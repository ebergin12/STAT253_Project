---
title: "Final Project Code ALL"
author: "Emily, Julian, Jacob, and Aristo"
date: '2022-11-20'
output: 
  html_document:
    df_print: paged
    toc: true
    code_download: true
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE, message=FALSE, warning=FALSE)
```

# Library Statements 

```{r}
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
library(stringr)
library(splitstackshape)
library(lubridate)
library(rpart.plot)
library(cluster)
library(forcats)
tidymodels_prefer()
library(probably) #install.packages('probably')
library(vip)
```

# Dataset

```{r}
imdb_top_1000 <- read_csv("~/Desktop/Statistical Machine Learning/R Files/Final Project/imdb_top_1000_CLEAN.csv")
```

## Data Cleaning

```{r}
imdb_clean <- imdb_top_1000 %>%
  cSplit("Genre", sep = ",", direction = "wide") %>%
  mutate(Gross = log(Revenue-Budget)) %>%
  select(-...15)

runtime_clean <- imdb_top_1000$Runtime %>%
  str_replace(" min", "") %>%
  as.numeric()

imdb_clean$Runtime <- runtime_clean

imdb_clean <- imdb_clean %>%
  filter(Gross != "-Inf") %>%
  drop_na(Gross, Budget)
```

## Data

```{r}
head(imdb_clean)
```

# Regression Models

## Linear Regression Model

### Creation of CV Folds

```{r}
data_cv10 <- vfold_cv(imdb_clean, v = 10)
```

### Model Spec, Recipes, and Workflows for Linear Regression Model

```{r}
# Model Spec

lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

# Recipe

full_lm_rec <- recipe(Gross ~ Runtime + IMDB_Rating + Meta_score + 
                   No_of_Votes + Genre_1, data = imdb_clean) %>%
    step_nzv(all_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors()) %>%
    step_naomit(Gross, Runtime, IMDB_Rating, Meta_score, No_of_Votes)

# Workflow

full_lm_wf <- workflow() %>%
    add_recipe(full_lm_rec) %>%
    add_model(lm_spec)
```

### Fit Preliminary Model

```{r}
full_lm_model <- fit(full_lm_wf, data = imdb_clean) 

full_lm_model %>% tidy()
```

### Obtain Evaluation Metrics for Preliminary Model

```{r}
full_lm_modelcv <- fit_resamples(full_lm_wf, resamples = data_cv10, metrics = metric_set(rmse, rsq, mae))

full_lm_modelcv %>%
  collect_metrics()
```

### Perform LASSO for Subset Selection

```{r}
# Lasso Model Spec with tune

lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%   # mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>%             
  set_mode('regression') 

# Recipe

data_rec_lasso <- recipe(Gross ~ Runtime + IMDB_Rating + Meta_score + 
                   No_of_Votes + Genre_1, data = imdb_clean) %>%
    step_nzv(all_predictors()) %>%                # removes variables with the same value (don't want duplicates)
    step_novel(all_nominal_predictors()) %>%      # important if you have rare categorical variables 
    step_normalize(all_numeric_predictors()) %>%  # standardization important step for LASSO
    step_dummy(all_nominal_predictors()) %>%      # creates indicator variables for categorical variables
    step_naomit(Gross, Runtime, IMDB_Rating,      # omit any NA values
                Meta_score, No_of_Votes)                            

# Workflow

lasso_wf_tune <- workflow() %>% 
  add_recipe(data_rec_lasso) %>%
  add_model(lm_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)

penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)),
  levels = 30)

tune_res <- tune_grid(
  lasso_wf_tune, 
  resamples = data_cv10, 
  metrics = metric_set(rmse, mae),
  grid = penalty_grid 
)

# Visualize Model Evaluation Metrics from Tuning

autoplot(tune_res) + theme_classic()

# Collect CV Metrics and Select Best Model

# Summarize Model Evaluation Metrics (CV)
lasso_mod <- collect_metrics(tune_res) %>%
  filter(.metric == 'rmse') %>%
  select(penalty, rmse = mean) 

best_penalty <- select_best(tune_res, metric = 'rmse') # choose penalty value

lasso_mod
```

### Fit Final Model

```{r}
# Fit Final Model

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow

final_fit <- fit(final_wf, data = imdb_clean)

lasso_fit <- fit_resamples(final_wf, resamples = data_cv10, metrics = metric_set(rmse, rsq, mae))

tidy(final_fit)

# Final ("best") model predictors and coefficients

final_fit %>% tidy() %>% filter(estimate != 0)
```

### Visualize Residuals

```{r}
lasso_mod_out <- final_fit %>%
    predict(new_data = imdb_clean) %>%
    bind_cols(imdb_clean) %>%
    mutate(resid = Gross - .pred)

ggplot(lasso_mod_out, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out, aes(x = Runtime, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out, aes(x = IMDB_Rating, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out, aes(x = No_of_Votes, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()
```

## GAM Using Splines

```{r}
# Build the GAM

gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
    Gross ~ s(Runtime) + IMDB_Rating + s(Meta_score) + s(No_of_Votes) + Genre_1,
    data = imdb_clean 
)

```

````{r}
# Diagnostics: Check to see if the number of knots is large enough (if p-value is low, increase number of knots)

gam_mod %>% pluck('fit') %>% mgcv::gam.check()

gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
# Visualize: Look at the estimated non-linear functions

gam_mod %>% pluck('fit') %>% plot()
```



```{r}
spline_rec <- recipe(Gross ~ Runtime + IMDB_Rating + Meta_score + 
                   No_of_Votes + Genre_1, data = imdb_clean) %>%
    step_nzv(all_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors()) %>%
    step_naomit(Gross) %>%
    step_ns(Runtime, deg_free = 2) %>%
    step_ns(No_of_Votes, deg_free = 6)


spline_rec %>% prep(imdb_clean) %>% juice()
```


```{r}
# Use the edf (round to integer) from above to create a recipe by adding step_ns() for the variables you want model with a non-linear relationship and do CV. Compare this model to one without any splines.

lm_spec_gam <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')


spline_wf <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(spline_rec)

cv_output <- fit_resamples( 
  spline_wf, # workflow
  resamples = data_cv10, # cv folds
  metrics = metric_set(mae,rmse,rsq)
)

cv_output %>% collect_metrics()

```

```{r}
new_mod <- spline_wf %>%
  fit(data = imdb_clean)

new_mod %>%
  tidy() 

new_mod %>% 
  tidy() %>% 
  arrange(desc(abs(statistic)))

new_modcv <- fit_resamples(spline_wf, resamples = data_cv10, metrics = metric_set(mae, rmse, rsq))
```



# Variable Importance for GAM

```{r}
# Lasso Model Spec with tune
gam_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 

# Recipe with standardization (!) --> just include all these always
data_rec <- recipe(Gross ~ Runtime + IMDB_Rating + Meta_score + 
                   No_of_Votes + Genre_1, data = imdb_clean) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value (so duplicates don't mess up model)
    step_novel(all_nominal_predictors()) %>% # important if you have rare categorical variables 
    step_normalize(all_numeric_predictors()) %>%  # super important standardization step for LASSO
    step_dummy(all_nominal_predictors()) %>%  # creates indicator variables for categorical variables
    step_naomit(Gross)

# Workflow (Recipe + Model)
lasso_wf_tune1 <- workflow() %>% 
  add_recipe(data_rec) %>%
  add_model(gam_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)),
  levels = 30)

tune_res1 <- tune_grid( # new function for tuning parameters
  lasso_wf_tune1, # workflow
  resamples = data_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

# Visualize Model Evaluation Metrics from Tuning
autoplot(tune_res1) + theme_classic()
```

# Collect CV Metrics and Select Best Model

```{r}

# Summarize Model Evaluation Metrics (CV)
collect_metrics(tune_res) %>%
  filter(.metric == 'rmse') %>% # or choose mae
  select(penalty, rmse = mean) 

best_penalty1 <- select_best(tune_res1, metric = 'rmse') # choose penalty value based on lowest mae or rmse
```

# Fit Final Model

```{r}

# Fit Final Model
final_wf1 <- finalize_workflow(lasso_wf_tune1, best_penalty) # incorporates penalty value to workflow

final_fit1 <- fit(final_wf1, data = imdb_clean)

tidy(final_fit1)
```

```{r}
# Final ("best") model predictors and coefficients

final_fit1 %>% tidy() %>% filter(estimate != 0)
```

# Visualize Residuals

```{r}

# Visualize Residuals
lasso_mod_out1 <- new_mod %>%
    predict(new_data = imdb_clean) %>%
    bind_cols(imdb_clean) %>%
    mutate(resid = Gross - .pred)


ggplot(lasso_mod_out1, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out1, aes(x = Runtime, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out1, aes(x = IMDB_Rating, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(lasso_mod_out1, aes(x = No_of_Votes, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()
```


# Comparing Model Performance

```{r}
full_lm_modelcv %>% collect_metrics() #OLS
```

```{r}
lasso_fit %>% collect_metrics() #LASSO
```

```{r}
new_modcv %>% collect_metrics() #GAM
```

# Bagging and Random Forests

```{r}
imdb_clean_class <- imdb_clean %>%
  mutate(success_ratio = Revenue/Budget) %>%
  mutate(flop = as.factor(ifelse(success_ratio > 2, 'FALSE', 'TRUE'))) %>%
  drop_na(flop, No_of_Votes,Runtime, IMDB_Rating,Meta_score,Genre_1)
```

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(trees = 1000, # Number of trees
           min_n = NULL,
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

# Recipe
data_rec <- recipe(flop ~ No_of_Votes + Runtime + IMDB_Rating + Meta_score + Genre_1,
                   data = imdb_clean_class) %>%
  step_naomit(flop, No_of_Votes, Runtime, IMDB_Rating, Meta_score, Genre_1)

# Create Workflows
data_wf_mtry3 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 3)) %>%
  add_recipe(data_rec) 

data_wf_mtry4 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 4)) %>%
  add_recipe(data_rec) 

data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5)) %>%
  add_recipe(data_rec)
```

```{r}
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry3 <- fit(data_wf_mtry3, data = imdb_clean_class)

set.seed(123) 
data_fit_mtry4 <- fit(data_wf_mtry4, data = imdb_clean_class)

set.seed(123)
data_fit_mtry5 <- fit(data_wf_mtry5, data = imdb_clean_class)
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          flop = truth,
          model = model_label
      )
}
```


```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry3,3, imdb_clean_class %>% pull(flop)),
    rf_OOB_output(data_fit_mtry4,4, imdb_clean_class %>% pull(flop)),
    rf_OOB_output(data_fit_mtry5,5, imdb_clean_class %>% pull(flop))
)


data_rf_OOB_output %>% 
    group_by(model) %>%
    accuracy(truth = flop, estimate = .pred_class)

data_rf_OOB_output %>% 
  group_by(model) %>%
  accuracy(truth = flop, estimate = .pred_class) %>%
  mutate(mtry = as.numeric(stringr::str_replace(model,'mtry',''))) %>%
  ggplot(aes(x = mtry, y = .estimate )) + 
  geom_point() +
  geom_line() +
  theme_classic()
```

### Evaluating the Forest

```{r}
rf_OOB_output(data_fit_mtry4,4, imdb_clean_class %>% pull(flop)) %>%
    conf_mat(truth = flop, estimate= .pred_class)
```

### Variable Importance

```{r}
# Impurity

model_output <-data_fit_mtry4 %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 10) + theme_classic() #based on impurity, 10 meaning the top 10

model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```

```{r}
# Permutation

model_output2 <- data_wf_mtry4 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = imdb_clean_class) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 10) + theme_classic()


model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
```
```{r}
ggplot(imdb_clean_class, aes(x = flop, y = No_of_Votes)) +
    geom_violin() + theme_classic()

ggplot(imdb_clean_class, aes(x = flop, y = Runtime)) +
    geom_violin() + theme_classic()

ggplot(imdb_clean_class, aes(x = flop, y = IMDB_Rating)) +
    geom_violin() + theme_classic()

ggplot(imdb_clean_class, aes(x = flop, y = Meta_score)) +
    geom_violin() + theme_classic()
```

# Logistic Regression

```{r}
set.seed(123)

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(flop ~ No_of_Votes + Runtime + IMDB_Rating + Genre_1,
                   data = imdb_clean_class)

# Workflow (Recipe + Model) for Full Log Model
log_wf <- workflow() %>%
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec)

# Fit Model
log_fit <- fit(log_wf, data = imdb_clean_class)

tidy(log_fit)
```


```{r}
log_fit %>% tidy() %>%
  mutate(OR = exp(estimate))
```

*Ask Bryan about relevel factor... do we need it?*

```{r}
# Creation of CV Folds
data_cv10_class <- vfold_cv(imdb_clean_class, v = 10)
```


# Recipes and Workflows

```{r}
full_log_wf <- workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_spec)
    
log_model <- fit(full_log_wf, data = imdb_clean_class) 

log_modelcv <- fit_resamples(full_log_wf, resamples = data_cv10_class, metrics = metric_set(accuracy,sens,yardstick::spec))

log_modelcv %>%
  collect_metrics()
```

```{r}
# Soft Predictions on Training Data
final_output <- log_fit %>% predict(new_data = imdb_clean_class, type='prob') %>% bind_cols(imdb_clean_class)

final_output %>%
  ggplot(aes(x = flop, y = .pred_TRUE)) +
  geom_boxplot()
```

```{r}
# Use soft predictions
final_output %>%
    roc_curve(flop,.pred_TRUE,event_level = 'second') %>%
    autoplot()
```

```{r}
# thresholds in terms of reference level
threshold_output <- final_output %>%
    threshold_perf(truth = flop, estimate = .pred_FALSE, thresholds = seq(0,1,by=.01)) 

# J-index v. threshold for no flop
threshold_output %>%
    filter(.metric == 'j_index') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'J-index', x = 'threshold') +
    theme_classic()
```

```{r}
threshold_output %>%
    filter(.metric == 'j_index') %>%
    arrange(desc(.estimate))
```

```{r}
# Distance v. threshold for not_spam

threshold_output %>%
    filter(.metric == 'distance') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'Distance', x = 'threshold') +
    theme_classic()
```

```{r}
threshold_output %>%
    filter(.metric == 'distance') %>%
    arrange(.estimate)
```

```{r}
log_metrics <- metric_set(accuracy,sens,yardstick::spec)

final_output %>%
    mutate(.pred_class = make_two_class_pred(.pred_FALSE, levels(flop), threshold = .12)) %>%
    log_metrics(truth = flop, estimate = .pred_class, event_level = 'second')

final_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_FALSE, levels(flop), threshold = .12)) %>%
  conf_mat(truth = flop, estimate = .pred_class)
```

```{r}
predict(log_fit, new_data = data.frame(No_of_Votes = 10000, Runtime = 112, IMDB_Rating = 9.8,
                                        Genre_1 = "Drama"), type = "prob"
)
```


```{r}
predict(log_fit, new_data = data.frame(No_of_Votes = 10000, Runtime = 112, IMDB_Rating = 9.8,
                                        Genre_1 = "Drama"), type = "class"
)
```

*Must manually calculate hard prediction*

### K-Means Clustering

```{r}
ggplot(imdb_clean, aes(x = Budget, y = Runtime)) + 
  geom_point() + theme_classic()

ggplot(imdb_clean, aes(x = Budget, y = Gross)) + 
  geom_point() + theme_classic()

ggplot(imdb_clean, aes(x = No_of_Votes, y = Runtime)) + 
  geom_point() + theme_classic()

ggplot(imdb_clean, aes(x = Gross, y = No_of_Votes)) + 
  geom_point() + theme_classic()
```


```{r}
# Select just the bill length and depth variables
imdb_sub <- imdb_clean %>%
    select(Budget, Runtime)

# Run k-means for k = centers = 3
set.seed(253)
kclust_k10 <- kmeans(scale(imdb_sub), centers = 10)

# Display the cluter assignments
kclust_k10$cluster

# Add a variable (kclust_3) to the original dataset 
# containing the cluster assignments
imdb_clean <- imdb_clean %>%
    mutate(kclust_10 = factor(kclust_k10$cluster))
```

```{r}
# Visualize the cluster assignments on the original scatterplot
imdb_clean %>%
  ggplot(aes(x = Budget, y = Runtime, color = kclust_10)) +
    geom_point() + theme_classic()
```

```{r}
# Data-specific function to cluster and calculate total within-cluster SS
imdb_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(imdb_sub), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, imdb_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

```{r}
kclust_k8 <- kmeans(scale(imdb_sub), centers = 8)

# Display the cluter assignments
kclust_k8$cluster

# Add a variable (kclust_3) to the original dataset 
# containing the cluster assignments
imdb_clean <- imdb_clean %>%
    mutate(kclust_8 = factor(kclust_k8$cluster))
```

```{r}
# Visualize the cluster assignments on the original scatterplot
imdb_clean %>%
  ggplot(aes(x = Budget, y = Runtime, color = kclust_8)) +
    geom_point() + theme_classic()
```

*Pick either k = 8 or k = 10 --> there are 7 genres with larger n and 13 total genres. Film noir is fancy way of saying crime drama (ex. Knives Out). One thing to note is a significant number of movies have Genre_2 listed as Family Mystery and Horror (and many have one of the main genres listed as genre 2 as well). What is a good way to summarize both Genre 1 and 2?*

```{r}
imdb_clean %>%
  count(Genre_1)

imdb_clean %>%
  count(Genre_2)
```

## Interpreting the Clusters

```{r}
imdb_clean %>%
  group_by(kclust_10) %>%
  summarize(across(c(Gross, Budget), mean))

imdb_clean %>%
  group_by(kclust_8) %>%
  count(Genre_1)

imdb_clean %>%
  group_by(kclust_8) %>%
  count(Genre_2)

imdb_clean%>%
  count(kclust_8)
```



### Hierarchial Clustering

```{r}
# Random subsample of 50 penguins
set.seed(253)
imdb_hc <- imdb_clean %>%
    slice_sample(n = 25)

# Select the variables to be used in clustering
imdb_hc_sub <- imdb_hc %>%
    select(Gross, Budget)

# Summary statistics for the variables
summary(imdb_hc_sub)

# Compute a distance matrix on the scaled data
dist_mat_scaled <- dist(scale(imdb_hc_sub))
```

```{r}
imdb_hc_avg <- hclust(dist_mat_scaled, method = "average")

# Plot dendrogram
plot(imdb_hc_avg)
```

```{r}

plot(imdb_hc_avg, labels = imdb_hc$Genre_1)

plot(imdb_hc_avg, labels = paste(imdb_hc$Genre_1, imdb_hc$Genre_2))
```

```{r}
imdb_clean <- imdb_clean %>%
    mutate(
        hclust_height = factor(cutree(imdb_hc_avg, h = 1.25)), # Cut at height (h) 3
        hclust_num = factor(cutree(imdb_hc_avg, k = 5)) # Cut into 6 clusters (k)
    )

plot(imdb_hc_avg, labels = imdb_clean$hclust_height)
```


