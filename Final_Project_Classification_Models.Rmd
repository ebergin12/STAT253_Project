---
title: 'STAT 253 Final Project - Classification Models'
author: "Jacob, Julian, Aristo, and Emily"
date: "October 31, 2022"
output: 
  html_document:
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE, message=FALSE, warning=FALSE)
```


Classification Question: Which predictor strongly influences movie revenue?

# Library Statements 

```{r}
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
library(stringr)
library(splitstackshape)
library(tidymodels)
library(lubridate)
library(rpart.plot)
library(cluster)
library(forcats)
tidymodels_prefer()
library(probably) #install.packages('probably')
library(vip)
```

# Read in Data

```{r}
imdb_top_1000 <- read_csv("imdb_top_1000_updated.csv")
```

# Data Cleaning

```{r}
imdb_clean <- imdb_top_1000 %>%
  cSplit("Genre", sep = ",", direction = "wide") %>%
  mutate(Gross = log(revenue))

runtime_clean <- imdb_top_1000$Runtime %>%
  str_replace(" min", "") %>%
  as.numeric()

imdb_clean$Runtime <- runtime_clean

imdb_clean_class <- imdb_clean %>%
  mutate(success_ratio = revenue/budget) %>%
  mutate(flop = as.factor(ifelse(success_ratio > 1.5, 'FALSE', 'TRUE'))) %>%
  filter(!is.na(flop))

```



# Bagging and Random Forests

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 4, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = NULL,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(flop ~ No_of_Votes, Runtime, IMDB_Rating, Meta_score, Genre, popularity, vote_average, vote_count,
                   data = imdb_clean_class) %>%
  step_naomit()

# Workflows
data_wf_mtry150 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 150)) %>%
  add_recipe(data_rec) 

## Create workflows for min_n = 10, 25, and 50

data_wf_mtry125 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 125)) %>%
  add_recipe(data_rec)

data_wf_mtry75 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 75)) %>%
  add_recipe(data_rec)

data_wf_mtry100 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 100)) %>%
  add_recipe(data_rec)
```

```{r}
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry150 <- fit(data_wf_mtry150, data = imdb_clean_class)

# Fit models for 10, 25, and 50
set.seed(123) 
data_fit_mtry125 <- fit(data_wf_mtry125, data = imdb_clean_class)

set.seed(123)
data_fit_mtry75 <- fit(data_wf_mtry75, data = imdb_clean_class)

set.seed(123) 
data_fit_mtry100 <- fit(data_wf_mtry100, data = imdb_clean_class)
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          flop = truth,
          label = model_label
      )
}

rf_OOB_output(data_fit_mtry150,150, imdb_clean_class %>% pull(flop))
```

```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry150,150, imdb_clean_class %>% pull(flop)),
    rf_OOB_output(data_fit_mtry125,125, imdb_clean_class %>% pull(flop)),
    rf_OOB_output(data_fit_mtry75,75, imdb_clean_class %>% pull(flop)),
    rf_OOB_output(data_fit_mtry100,100, imdb_clean_class %>% pull(flop))
)


data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = flop, estimate = .pred_class)
```

### Evaluating the Forest

```{r}
rf_OOB_output(data_fit_mtry125,125, imdb_clean_class %>% pull(flop)) %>%
    conf_mat(truth = class, estimate= .pred_class)
```

### Variable Importance

```{r}
# Impurity

model_output <-data_fit_mtry125 %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 10) + theme_classic() #based on impurity, 10 meaning the top 10

model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```

```{r}
# Permutation

model_output2 <- data_wf_mtry125 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = imdb_clean_class) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 10) + theme_classic()


model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
```

# Logistic Regression

```{r}
set.seed(123)

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')
# Recipe
logistic_rec <- recipe(flop ~ IMDB_Rating, No_of_Votes, gross,
                   data = imdb_clean_class)
# Workflow (Recipe + Model)
log_wf <- workflow() %>%
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec)
# Fit Model
log_fit <- fit(log_wf, data = imdb_clean_class)

tidy(log_fit)
```


```{r}
log_fit %>% tidy() %>%
  mutate(OR = exp(estimate))
```
```{r}
predict(log_fit, new_data = data.frame(IMDB_Rating = 5, No_of_Votes = 500000, gross = 18), type = "prob"
)
```


```{r}
predict(log_fit, new_data = data.frame(IMDB_Rating = 5, No_of_Votes = 500000, gross = 18), type = "class"
)
```

A movie with an IMDB Rating of 5 with 500,000 votes and gross of 18 will be a flop.

# Model Reflection


Classification - Methods

Indicate at least 2 different methods used to answer your classification research question.

Random Forests and Logistic Regression

Describe what you did to evaluate the models explored.

We underwent a random forests classification model utilizing OOB evaluation metrics. We also utilized a logistic regression model to create hard predictions.

Indicate how you estimated quantitative evaluation metrics.

Included in the above code is OOB importance measures and logistic regression test cases.

Describe the goals / purpose of the methods used in the overall context of your research investigations.

We are attempting to determine if a movie will flop based on its difference in budget and gross.

Classification - Results

Summarize your final model and justify your model choice (see below for ways to justify your choice).



Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)
Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.

?

Classification - Conclusions - Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? - If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.


/

